D:\training\Anaconda3\Scripts\activate aiml 
jupyter notebook --notebook-dir='D:/handson'


#Step-1  , Run - Shift+Enter 
print("Hello world!")

#Step-2
2 + 2

#Step-3: access last result 
_ * 3

#step-4: execute shell command 
!dir

#step-5: Magic command 
#https://ipython.readthedocs.io/en/stable/interactive/magics.html
%lsmagic

#Step-51:Use 
%pwd  #look at the current work dir
%cd   #change to the dir you want 
    
#Step-6: Use cell magic 
%%writefile test.txt
Hello world!


# Let's check what this file contains.
with open('test.txt', 'r') as f:
    print(f.read())


#find more information about any command by adding ? after it
#eg running external program 
%run?

#Step-7 : Create Markdown cell
press Esc and then press M (can be done with Menu as well)
    ##Command 
    Toggle between edit and command mode with Esc and Enter, respectively.
    Once in command mode:
        Scroll up and down your cells with your Up and Down keys.
        Press A or B to insert a new cell above or below the active cell.
        M will transform the active cell to a Markdown cell.
        Y will set the active cell to a code cell.
        D + D (D twice) will delete the active cell.
        Z will undo cell deletion.
        Hold Shift and press Up or Down to select multiple cells at once.
            With multple cells selected, Shift + M will merge your selection.
    Ctrl + Shift + -, in edit mode, will split the active cell at the cursor.
    You can also click and Shift + Click in the margin to the left of your cells to select them.
Markdown  can be written with HTML tags as well as below github commands 
To edit markdown cell, double click it 
To show, click run or Shift+ENTER (there should not be any whitespace in the begining)

# This is a level 1 heading
## This is a level 2 heading
This is some plain text that forms a paragraph.
Add emphasis via **bold** and __bold__, or *italic* and _italic_.

Paragraphs must be separated by an empty line.

* Sometimes we want to include lists.
 * Which can be indented.

1. Lists can also be numbered.
2. For ordered lists.

[It is possible to include hyperlinks](https://www.example.com)

Inline code uses single backticks: `foo()`, and code blocks use triple backticks:

```
bar()
```

Or can be intented by 4 spaces:

    foo()

And finally, adding images is easy: ![Alt text](https://www.example.com/image.jpg)

Also mathematical equation [check Latex link](http://ctan.math.ca/tex-archive/info/symbols/comprehensive/SYMLIST) 
$$\hat{f}(\xi) = \int_{-\infty}^{+\infty} f(x) \, \exp \left(-2i\pi x \xi \right) dx $$


#Step-9: Sophisticated display 
from IPython.display import HTML, SVG, YouTubeVideo

#Step-10. HTML table dynamically with Python
HTML('''
<table style="border: 2px solid black;">
''' +
     ''.join(['<tr>' +
              ''.join([f'<td>{row},{col}</td>'
                       for col in range(5)]) +
              '</tr>' for row in range(5)]) +
     '''
</table>
''')

#create an SVG graphics dynamically:

SVG('''<svg width="600" height="80">''' +
    ''.join([f'''<circle
              cx="{(30 + 3*i) * (10 - i)}"
              cy="30"
              r="{3. * float(i)}"
              fill="red"
              stroke-width="2"
              stroke="black">
        </circle>''' for i in range(10)]) +
    '''</svg>''')



#display a Youtube video 
YouTubeVideo('VQBZ2MqWBZI')



#Notebooks are saved as structured text files (JSON format), 
nbconvert is a tool that can convert notebooks to other formats: raw text, 
Markdown, HTML, LaTeX/PDF

There is a free online service, nbviewer Lets renders notebook into HTML 
check https://nbviewer.jupyter.org/ 
When you give it a URL, it fetches the notebook from that URL, converts it to HTML, 
and serves that HTML to you.
nbviewer only supports launching notebooks stored on GitHub or as Gists on Binder. 
Binder(https://mybinder.org)does support other providers directly on the mybinder.org site.

Locally to get the same functionality (and more)
Check help 
$ jupyter nbconvert 

#To html 
$ jupyter nbconvert first.ipynb

#display this html 
from IPython.display import IFrame
IFrame('first.html', 600, 200)

To save jupyter nootbook as pdf, install pandoc(anaconda brings it) and MikTex 
https://github.com/jgm/pandoc/releases/latest
https://miktex.org/download (check where miktex-tex.exe)
then start jupyter - Note first time while saving, it may download many files, 
hence may fail couple of time, but keep on trying 

$ jupyter nbconvert --to pdf first.ipynb




##matplotlib 
t = np.arange(0., 5., 0.2)
figure, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=False, figsize=(15,15))
figure.suptitle('multiplot', fontsize=16) #called suptitle instead of title 
ax1.plot(t, t, 'r-')  
ax1.set_title('Straight line')
ax2.scatter(t, t**2, color='b')
ax2.set_xlabel('x axis label')
ax2.set_ylabel('y axis label')
ax2.set_title('Simple plot')
ax2.legend(['Square',])

ax2.axis([0, 5, 0,25]) #Set the axis, [xmin, xmax, ymin, ymax] or 'off' or 'equal' or 'scaled' or 'tight' etc 
ax2.set_axis_off() #Turn off the axis. 
ax2.set_axis_on()  # Turn on the axis.
#or 
ax2.set_ylim(0,25)
ax2.set_xlim(0, 5) 

#in data cordinates 
ax2.text(2, 4, r'$\mu=100,\ \sigma=15$')  #Any text 
ax2.annotate('local max', xy=(2, 1), xytext=(3, 1.5),arrowprops=dict(facecolor='black', shrink=0.05),) #xytext=location of text, xy=location for annotation
ax2.arrow(4,4,1,1)  #(x, y, dx, dy, **kwargs)    #Add an arrow to the axes.
ax2.grid(b=True, color='r', linestyle='-', linewidth=.5)
#ax2.set_label(s)       #Set the label to s for auto legend.
 

ax2.set_yscale('linear') #log, symlog, logit
ax2.set_xscale('linear')
ax2.set_visible(True)     #Set the artist's visibility.
#ax2.set_zorder(level)  #Set the zorder for the artist. Artists with lower zorder values are drawn first.

# ax2.set_xticks(ticks, minor=False)     #Set the x ticks with list of ticks(List of tick locations.)
# ax2.set_xticklabels(labels, fontdict=None, minor=False, **kwargs)#Set the x-tick labels with list of string labels.
# ax2.set_yticks(ticks, minor=False)#Set the y ticks with list of ticks(List of tick locations.)
# ax2.set_yticklabels(labels, fontdict=None, minor=False, **kwargs)#Set the y-tick labels with list of strings labels.

# ax2.xaxis_date(tz=None)  #Sets up x-axis ticks and labels that treat the x data as dates.
# ax2.yaxis_date(tz=None)  #Sets up y-axis ticks and labels that treat the y data as dates.
# ax2.minorticks_off()     #Remove minor ticks from the axes.
# ax2.minorticks_on()      #Remove minor ticks from the axes.

#then show 
plt.show()

##numpy 
$$f(x,y) = sin^2(3\pi x)+(x−1)^2(1+sin^2(3\pi y))+(y−1)^2(1+sin^2(2\pi y))$$



##Pandas 
A. Daily attendance of bike tracks
url = "https://raw.githubusercontent.com/ndas1971/Misc/master/data/bikes.csv"

1. Read 
2. Check head 
3. Check summary statistics 
4. plot the daily attendance of two tracks, 'Berri1', 'PierDup'
5. Check index , index contain date, weekday_name works 
6. Get sum of all attendance as a function of the weekday
7. Display this in figure , what is the inference?

df = pd.read_csv(url, index_col='Date', parse_dates=True, dayfirst=True)


from ipywidgets import interact,interact_manual

@interact(n=(1, 30))
def plot(n=5):
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))
    df['Berri1'].rolling(window=n).mean().plot(ax=ax)
    ax.set_ylim(0, 7000)
    plt.show()
  
#@interact_manual decorator  provides a button to call the function manually
@interact_manual(n=(1, 30))
def plot(n=5):
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))
    df['Berri1'].rolling(window=n).mean().plot(ax=ax)
    ax.set_ylim(0, 7000)
    plt.show()


B. Titanic - https://www.kaggle.com/c/titanic/data
Based on condition, whether, somebody is survived or not 
1.Load the data
2. Which gender survived more 
3. Does it depend on pclass?
4. Not clear, can we see % of survival of each gender and pclass 
What inference? 


C. Roger Federer database 
Each row corresponds to a ATP match played by Roger Federer.
player = 'Roger Federer'
url = "https://raw.githubusercontent.com/ndas1971/Misc/master/data/federer.csv"

1. Read and check data 
2. How many % of matched won by our player? ('winner')
3. Proportion of double faults wrt total points in each match 
This number is an indicator of the player's state of mind, his level of self-confidence, 
his willingness to take risks while serving, and other parameters.
columns:
'player1 double faults' and 'player1 total points total'
Display simple stats of above 
4. Average Win per surface 
5. Display the proportion of double faults as a function of the tournament date, 'start date'
Trend: display average double faults in each year 
                 
                 
                 




###hypothesis testing 

#problem-1 
Here, we flip a coin n times and we observe h heads. 
We want to know whether the coin is fair (null hypothesis). 

This discrete distribution us characterized by Bernoulli distribution by B(q) with the unknown parameter q
https://en.wikipedia.org/wiki/Bernoulli_distribution for more information.
A Bernoulli variable is: 0 (tail) with probability 1−q 1 (head) with probability q

Let's suppose that after n=100 flips, we get h=61 heads. 
We choose a significance level of 0.05: is the coin fair or not? 
Our null hypothesis is: the coin is fair (q=1/2)

Binomial test H0: probability of success given x(61) out of n(100) trials is p(.5)


##Problem 
A soft drink company has invented a new drink, 
and would like to find out if it will be as popular as the existing favorite drink. 
For this purpose, its research department arranges 18 participants for taste testing. 
Each participant tries both drinks in random order before giving his or her opinion. 

It turns out that 5 of the participants like the new drink better, 
and the rest prefer the old one. 
Are two drinks equally popular? 

##Another Problem 
A car manufacturer claims that no more than 10% of their cars are unsafe.
15 cars are inspected for safety, 3 were found to be unsafe. Test the
manufacturer's claim:

##Another Problem 
A biologist runs an experiment in which there are three groups of plants. 
Group 1 has 16 plants, group 2 has 15 plants, and group 3 has 17 plants. 
Each plant produces a number of seeds. The seed counts for each group are:
Ho: g1, g2, g3 have same median 
g1 = [10, 14, 14, 18, 20, 22, 24, 25, 31, 31, 32, 39, 43, 43, 48, 49]
g2 = [28, 30, 31, 33, 34, 35, 36, 40, 44, 55, 57, 61, 91, 92, 99]
g3 = [0, 3, 9, 22, 23, 25, 25, 33, 34, 34, 40, 45, 46, 48, 62, 67, 84]

##Another problem 
Gender  Right handed    Left handed     Total
Male        43          9               52
Female      44          4               48 
Total       87          13              100 

H0: male-female's  right or left handed are independent or not?

##Another Problem  
        Atlantic  Indian
whales     8        2
sharks     1        5
H0: whales-sharks from Atlantic and Indian ocean are independent

##Another Problem 
player = 'Roger Federer'
url = "https://raw.githubusercontent.com/ndas1971/Misc/master/data/federer.csv"

Each row corresponds to a match, 
and the 70 columns contain many player characteristics during that match:

1. Look at the proportion of points won, and the (relative) number of aces wrt total points
Draw these two proportions
columns: 
'player1 total points total'
'player1 total points won'
'player1 aces'
       
2.If the two variables were independent, we would not see any trend in the cloud of points.
On this plot, it is a bit hard to tell. 
Use  pandas to compute a coefficient correlation.
What is the inference?
Is that "the more aces in a match, the more points the player wins"

3.Determine if there is a statistically significant correlation between these variables, 
useing a chi-squared test of the independence of variables in a contingency table. 
(requires frequency)
#Example 

import numpy as np
import pandas as pd
import scipy.stats as st
import matplotlib.pyplot as plt
%matplotlib inline

player = 'Roger Federer'
df = pd.read_csv("https://raw.githubusercontent.com/ndas1971/Misc/master/data/federer.csv",
                 parse_dates=['start date'],
                 dayfirst=True)
                 
                 

#Each row corresponds to a match, 
#and the 70 columns contain many player characteristics during that match:
print(f"Number of columns: {len(df.columns)}")
df[df.columns[:4]].tail()

#look at the proportion of points won, and the (relative) number of aces:
npoints = df['player1 total points total']
points = df['player1 total points won'] / npoints
aces = df['player1 aces'] / npoints

fig, ax = plt.subplots(1, 1)
ax.plot(points, aces, '.')
ax.set_xlabel('% of points won')
ax.set_ylabel('% of aces')
ax.set_xlim(0., 1.)
ax.set_ylim(0.)

#If the two variables were independent, we would not see any trend in the cloud of points.
#On this plot, it is a bit hard to tell. 
#Let's use pandas to compute a coefficient correlation.
df_bis = pd.DataFrame({'points': points, 'aces': aces}).dropna()
df_bis.tail()

#compute the Pearson's correlation coefficient 
#A correlation of ~0.26 seems to indicate a positive correlation between our two variables. 
#In other words, the more aces in a match, the more points the player wins 
df_bis.corr()


#to determine if there is a statistically significant correlation between the variables, 
#we use a chi-squared test of the independence of variables in a contingency table. 

# we binarize our variables. Here, the value corresponding to the number of aces is True 
#if the player is serving more aces than usual in a match, and False otherwise:
df_bis['result'] = (df_bis['points'] >  df_bis['points'].median())
df_bis['manyaces'] = (df_bis['aces'] >  df_bis['aces'].median())

#create a contingency table, with the frequencies of all four possibilities 
#(True and True, True and False, and so on):
pd.crosstab(df_bis['result'], df_bis['manyaces'])

#compute the chi-squared test statistic and the associated p-value. 
#The null hypothesis is the independence between the variables. 
#second result is the p-value:
st.chi2_contingency(_)

#The p-value is much lower than 0.05, so we reject the null hypothesis 
#and conclude that there is a statistically significant correlation 
#between the proportion of aces and the proportion of points won in a match in this dataset.

#Correlation does not imply causation(ie cause-effect)
#High aces causes high win , could be some external factors influencing both variables. 
#See https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation 

###Titanic Data 
https://www.kaggle.com/c/titanic/data
0. Understand data, download only train.csv 
A. Do below in pandas 
1. Read data 
2. Check head , datatypes , columns
3. How many rows are present?
4. display all rows and columns from 'name' to  'sibsp' inclusive
5. How many NaN are present in 'fare' and 'embarked' 
6. Replace any NaN of 'fare' and 'embarked'  by statistical mode of that column 
7. drop cabin 
8. check any row containing any null 
9. Display first row of above , can you find which column is still null 
10.Setup the data 
a) create 'Title' column (take title from 'name')
b) Create 'LastName' column from 'name' 
c) Create  'FamilySize' which is equal to 'sibsp' + 'parch' + 1
d) Create 'FamilyID' which concatenation of 'LastName' and 'FamiliySize'
e) if family size is <=2 , make 'FamilyID' as 'Small_Family'
f) Create 'AgeOriginallyNaN' as 0 or 1 based on age is NaN or present 
g) For each title, get median age , and rename 'age' column to AgeFilledMedianByTitle
Create a new DataFrame (would be used for merging) of above 
g)Create 'final_age' which would contain 'age' if not NaN else value of 'AgeFilledMedianByTitle'
Hint: Use df.merge(right,...) and then use df.apply 
g.1)check that all final_age is correctly filled 
  
11. drop age column 
12.Drop any row which contain any NaN 


###SkLearn Imports 
from sklearn.pipeline import * 
from sklearn.naive_bayes import * 
from sklearn.cluster import *  
from sklearn.covariance import *  
from sklearn.cross_decomposition import *  
from sklearn.datasets import *  
from sklearn.decomposition import *  
from sklearn.ensemble import *  
from sklearn.feature_extraction import *  
from sklearn.feature_extraction.text import *  
from sklearn.feature_selection import *  
from sklearn.gaussian_process import *  
from sklearn.linear_model import *  
from sklearn.manifold import *  
from sklearn.metrics import *  
from sklearn.mixture import *  
from sklearn.model_selection import *  
from sklearn.neighbors import *  
from sklearn.neural_network import *  
from sklearn.preprocessing import *  
from sklearn_pandas import DataFrameMapper
from sklearn.svm import *  
from sklearn.tree import *  

import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt


##PCA 
def draw_pca(X_r, y, target_names):
    plt.figure()
    colors = ['navy', 'turquoise', 'darkorange']
    lw = 2
    #[0,1,2] are three Names 
    for color, i, target_name in zip(colors, [0, 1, 2], target_names):
        #draw 2 pca features for each y = target_names,3 species 
        plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw, label=target_name)
    plt.legend(loc='best', shadow=False, scatterpoints=1)
    plt.title('PCA of IRIS dataset')
    plt.show()
    
fig, (ax1,ax2) = plt.subplots(2,1)
draw_pca(X_r, y, target_names, ax1, 'PCA of IRIS dataset - 2 components variance ')
draw_pca(X, y, target_names, ax2, 'Original IRIS dataset - full variance')
fig.tight_layout()
fig.show()

###SVM 
iris = load_iris()
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
svr1 = SVC(gamma='auto')
#sklearn 
clf = GridSearchCV(svr1, parameters, verbose=0, cv=5) #or RandomizedSearchCV
clf.fit(iris.data, iris.target)
#clf.best_estimator_.score(iris.data, iris.target)  #(X, y)
scores = cross_val_score(clf.best_estimator_, iris.data, iris.target, scoring='accuracy', cv=10)
np.mean(scores), np.std(scores)
clf.best_estimator_.support_vectors_  #27 vectors from input taken as support vectors 
#support vectors are hyperplans best separating the data points 

###DT 
#Code 
iris = datasets.load_iris()
clf = DecisionTreeClassifier(criterion='entropy')

vals = cross_val_score(clf, iris.data, iris.target, cv=10)
print("cross validated score %3.2f +/- %3.2f" % (vals.mean(),vals.std()) )


X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)
clf = clf.fit(X_train, y_train)
predicted = clf.predict(X_test)
print("Training Accuracy",accuracy_score(y_train, clf.predict(X_train)))
print("Test data Accuracy",accuracy_score(y_test, predicted))

print("the probability of each class can be predicted(test data)")
print(clf.predict_proba(X_test)[:5])

print("We can get feature importance from DT algorithm")
[print(i,":", j) for i,j in zip(iris.feature_names,clf.feature_importances_) ]

import graphviz 
dot_data = export_graphviz(clf, out_file=None, 
                         feature_names=iris.feature_names,  
                         class_names=iris.target_names,  
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = graphviz.Source(dot_data)  
#graph.render('iris_entropy') #'iris.pdf'
graph  #Jupyter handles them directly 
print(""" 
Each node 
    Condition on One feature 
    entropy/gini = value at this node 
    samples = How many samples(or subsamples) is used for this condition
    value = ndarray([[class_1_count, class_2_count, ....]])
    whereas class_X_count = no of samples for that class 
    with that condition ie samples = SUM of value
    class = class from Majority 

Left Node = True of node condition 
    Another feature condition and it's data 
Right node = False of node condition 
    Another feature condition and it's data

Objective is to go down the tree by decreasing entropy/gini 
(entropy/gini is highest if all class_X_count = equal)
such that only one class is seen at leaf node 
""")


###Various kinds of Regressions 

import time 
data = load_boston()
X, y = data.data, data.target 
CV= KFold(n_splits=5,shuffle=True)
X_train, X_test, y_train, y_test = train_test_split(X,y) #random splitting 

clf1 = DecisionTreeRegressor()
clf2 = RandomForestRegressor(n_estimators=100)
clf3 = ExtraTreesRegressor(n_estimators=100)
clf4 =  AdaBoostRegressor(n_estimators=100)
clf5 =  GradientBoostingRegressor(n_estimators=100)
clf6 = SVR(kernel='rbf')
clf7 = StackingRegressor( estimators=[
                ('lr', RidgeCV()),
                ('svr', LinearSVR(random_state=42))],
                final_estimator=RandomForestRegressor(n_estimators=10))
names = [ 'DT', 'RF', 'ER', 'Ada', 'GBM', 'SVR', "Stack"]
pipe = Pipeline([('reg', clf1)])

grid = dict(
 reg=[clf1,clf2, clf3, clf4, clf5, clf6, clf7], 
)
search = GridSearchCV(pipe, param_grid=grid)
search.fit(X_train,y_train)
search.best_estimator_
search.best_params_
search.score(X_test, y_test)#.87 
#Additional 
search.cv_results_['params'] = names 
search.cv_results_['param_clf'] = names 
search.cv_results_['param_reg'] = names 
df = pd.DataFrame(search.cv_results_)
df


#FImport 
data = load_boston()
X, y = data.data, data.target 
X_train, X_test, y_train, y_test = train_test_split(X,y) #random splitting 
clf5 =  GradientBoostingRegressor(n_estimators=100)
clf5.fit(X_train,y_train)
df2 = pd.DataFrame(  
    clf5.feature_importances_,
    index=data.feature_names, columns=['FI'])
df2 = df2.reset_index().sort_values(by=['FI'],ascending=False)   
df2.index = df2['index']
df2.drop('index', axis=1, inplace=True)
df2['FI'] = df2.FI.apply(lambda e: "%d%%" %(e*100,))
#Or 
from yellowbrick.model_selection import *

data = load_boston()
X, y = data.data, data.target 
clf5 =  GradientBoostingRegressor(n_estimators=100)
viz = FeatureImportances(clf5, labels=data.feature_names)
viz.fit(X, y)
viz.show()